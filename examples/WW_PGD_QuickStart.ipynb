{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CalculatedContent/WW_PGD/blob/main/examples/WW_PGD_QuickStart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_lx6HuqxpkM",
        "outputId": "f3940c13-7880-48f3-878b-fb9c46bb4ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'WW_PGD' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/CalculatedContent/WW_PGD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip uninstall -y ww_pgd\n",
        "!{sys.executable} -m pip install -e /content/WW_PGD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsXe-xSXQfR7",
        "outputId": "d66ad40b-ccc6-48e9-8b4b-528b2f01fbbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping ww_pgd as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mObtaining file:///content/WW_PGD\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from ww_pgd==0.1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from ww_pgd==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: weightwatcher in /usr/local/lib/python3.12/dist-packages (from ww_pgd==0.1.0) (0.7.6)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->ww_pgd==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->ww_pgd==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->ww_pgd==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->ww_pgd==0.1.0) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->ww_pgd==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from weightwatcher->ww_pgd==0.1.0) (3.10.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from weightwatcher->ww_pgd==0.1.0) (0.2.1)\n",
            "Requirement already satisfied: powerlaw in /usr/local/lib/python3.12/dist-packages (from weightwatcher->ww_pgd==0.1.0) (1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from weightwatcher->ww_pgd==0.1.0) (1.6.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from weightwatcher->ww_pgd==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from weightwatcher->ww_pgd==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->ww_pgd==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->ww_pgd==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->ww_pgd==0.1.0) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->weightwatcher->ww_pgd==0.1.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->weightwatcher->ww_pgd==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->weightwatcher->ww_pgd==0.1.0) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->weightwatcher->ww_pgd==0.1.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->weightwatcher->ww_pgd==0.1.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->weightwatcher->ww_pgd==0.1.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->weightwatcher->ww_pgd==0.1.0) (3.2.5)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.12/dist-packages (from matplotlib-inline->weightwatcher->ww_pgd==0.1.0) (5.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from powerlaw->weightwatcher->ww_pgd==0.1.0) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->weightwatcher->ww_pgd==0.1.0) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->weightwatcher->ww_pgd==0.1.0) (3.6.0)\n",
            "Building wheels for collected packages: ww_pgd\n",
            "  Building editable for ww_pgd (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ww_pgd: filename=ww_pgd-0.1.0-0.editable-py3-none-any.whl size=4214 sha256=6ca099935132a5c59cb27f15e09dd6f264359cdfc3803aa58ca2278632b2c350\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l50owbnp/wheels/3b/3c/33/0d2a4467bbe2ae0dfe63d9943a304711b73f2ba07b2dc4f5e4\n",
            "Successfully built ww_pgd\n",
            "Installing collected packages: ww_pgd\n",
            "Successfully installed ww_pgd-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#restart Google colab session first\n",
        "\n",
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)\n"
      ],
      "metadata": {
        "id": "FbONiCMQ5KiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ww_pgd\n",
        "print(\"Imported from:\", ww_pgd.__file__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfTRKpth3o8m",
        "outputId": "da507567-f0f2-4bc3-a168-004d907627ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported from: /content/WW_PGD/ww_pgd/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QuickStart (trivial example)"
      ],
      "metadata": {
        "id": "0jpFBEEriacj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import ww_pgd\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Data loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "train_ds = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "\n",
        "# Model\n",
        "model = nn.Linear(28 * 28, 10).to(device)\n",
        "\n",
        "# Optimizers\n",
        "base_opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "cfg = ww_pgd.WWTailConfig(warmup_epochs=0, ramp_epochs=5)\n",
        "opt = ww_pgd.WWPGDWrapper(model, base_opt, cfg)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        xb = xb.view(xb.size(0), -1)  # flatten\n",
        "\n",
        "        loss = F.cross_entropy(model(xb), yb)\n",
        "        opt.zero_grad(set_to_none=True);\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    # epoch-boundary spectral projection\n",
        "    opt.apply_tail_projection(epoch=epoch, num_epochs=num_epochs)\n",
        "    print(f\"epoch {epoch+1}/{num_epochs} done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7WUAZ6PPNTs",
        "outputId": "4d67fc96-87d5-4d0a-8a86-f05f6bf3fca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 11.0MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 189kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.48MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 22.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMALL N PL FIT\n",
            "epoch 1/10 done\n",
            "SMALL N PL FIT\n",
            "epoch 2/10 done\n",
            "SMALL N PL FIT\n",
            "epoch 3/10 done\n",
            "SMALL N PL FIT\n",
            "epoch 4/10 done\n",
            "SMALL N PL FIT\n",
            "epoch 5/10 done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import weightwatcher as ww\n",
        "watcher = ww.WeightWatcher(model=model)\n",
        "details = watcher.analyze(detX=True, randomize=False, plot=True)\n",
        "details"
      ],
      "metadata": {
        "id": "kH9n7HgpXwVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PBffoDcJPNc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real Test: Train a 3-layer MLP on FashionMNIST"
      ],
      "metadata": {
        "id": "HksHDjaHZwto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# COMPLETE WORKING COLAB CELL (MNIST)\n",
        "# 5x Baseline vs 5x ww_pgd on MNIST (35 epochs)\n",
        "# Logs + plots: plain-test AND augmented-test accuracy per epoch\n",
        "# Plus WW metric plots (alpha/xmin/detX_num + detX_num - num_pl_spikes)\n",
        "# ======================================================\n",
        "\n",
        "# If you're using a local editable install already, you can comment these out.\n",
        "!pip -q install weightwatcher\n",
        "# If ww_pgd is not installed in your Colab kernel yet, uncomment:\n",
        "# !pip -q install git+https://github.com/CalculatedContent/WW_PGD.git\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "\n",
        "import weightwatcher as ww\n",
        "import ww_pgd\n",
        "\n",
        "# -----------------------------\n",
        "# Global experiment config\n",
        "# -----------------------------\n",
        "NUM_EPOCHS = 35\n",
        "N_RUNS = 5\n",
        "\n",
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "pin_memory = device == \"cuda\"\n",
        "num_workers = 2\n",
        "\n",
        "# -----------------------------\n",
        "# MNIST dataset + DataLoaders\n",
        "# -----------------------------\n",
        "batch_size = 128\n",
        "\n",
        "plain_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_ds = datasets.MNIST(root=\"./data\", train=True,  download=True, transform=plain_transform)\n",
        "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=plain_transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory)\n",
        "\n",
        "# Augmented evaluation transform (mild, in-distribution for MNIST)\n",
        "# NOTE: No horizontal flip for digits.\n",
        "aug_eval_transform = transforms.Compose([\n",
        "    transforms.RandomRotation(7),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.03, 0.03)),\n",
        "    transforms.GaussianBlur(3, sigma=(0.1, 0.3)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "test_aug_ds = datasets.MNIST(root=\"./data\", train=False, download=True, transform=aug_eval_transform)\n",
        "test_aug_loader = DataLoader(test_aug_ds, batch_size=batch_size, shuffle=False,\n",
        "                             num_workers=num_workers, pin_memory=pin_memory)\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluation\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb, reduction=\"sum\")\n",
        "        total_loss += loss.item()\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        total_correct += (preds == yb).sum().item()\n",
        "        total_samples += yb.size(0)\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "# -----------------------------\n",
        "# 3-layer MLP\n",
        "# -----------------------------\n",
        "class MLP3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 512),  # FC1\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),    # FC2\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),     # FC3\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# -----------------------------\n",
        "# WeightWatcher logging\n",
        "# -----------------------------\n",
        "def log_weightwatcher(model, epoch: int) -> pd.DataFrame:\n",
        "    watcher = ww.WeightWatcher(model=model)\n",
        "    details = watcher.analyze(detX=True, randomize=False, plot=False)\n",
        "    details[\"epoch_id\"] = epoch\n",
        "    return details\n",
        "\n",
        "# -----------------------------\n",
        "# Single run (baseline or ww_pgd)\n",
        "# -----------------------------\n",
        "def train_one_run(run_id: int, use_ww_pgd: bool, num_epochs: int = NUM_EPOCHS):\n",
        "    torch.manual_seed(123 + run_id)\n",
        "    np.random.seed(123 + run_id)\n",
        "\n",
        "    model = MLP3().to(device)\n",
        "    base_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "    ww_logs_local: List[pd.DataFrame] = []\n",
        "\n",
        "    if use_ww_pgd:\n",
        "        cfg = ww_pgd.WWTailConfig(\n",
        "            enable_tail_pgd=True,\n",
        "            min_tail=5,\n",
        "            q=1.0,\n",
        "            blend_eta=0.5,\n",
        "            cayley_eta=0.25,\n",
        "            use_detx=True,\n",
        "            warmup_epochs=0,\n",
        "            ramp_epochs=5,\n",
        "            enable_trap_pgd=False,\n",
        "            trap_blend_eta=0.5,\n",
        "            trap_tw_k=2.0,\n",
        "            trap_min_spikes=1,\n",
        "            verbose=False,\n",
        "        )\n",
        "        opt = ww_pgd.WWPGDWrapper(\n",
        "            model=model,\n",
        "            base_optimizer=base_optimizer,\n",
        "            tail_config=cfg,\n",
        "            apply_every_epochs=1,\n",
        "            ww_logs=None,  # logging separately below\n",
        "        )\n",
        "        ww_opt = opt\n",
        "    else:\n",
        "        opt = base_optimizer\n",
        "        ww_opt = None\n",
        "\n",
        "    mode = \"ww_pgd\" if use_ww_pgd else \"baseline\"\n",
        "    print(f\"\\n[Run {run_id}] Mode={mode}\")\n",
        "\n",
        "    test_accs = []\n",
        "    aug_accs  = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits, yb)\n",
        "\n",
        "            # optimizer API compatibility\n",
        "            if hasattr(opt, \"zero_grad\"):\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "            else:\n",
        "                base_optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            if hasattr(opt, \"step\"):\n",
        "                opt.step()\n",
        "            else:\n",
        "                base_optimizer.step()\n",
        "\n",
        "        # Apply WW-PGD at epoch boundary (requires num_epochs)\n",
        "        if use_ww_pgd and ww_opt is not None:\n",
        "            ww_opt.apply_tail_projection(epoch=epoch, num_epochs=num_epochs)\n",
        "\n",
        "        # Log WW metrics\n",
        "        ww_details = log_weightwatcher(model, epoch=epoch)\n",
        "        ww_logs_local.append(ww_details)\n",
        "\n",
        "        # Evaluate plain + augmented\n",
        "        _, test_acc = evaluate(model, test_loader)\n",
        "        _, aug_acc  = evaluate(model, test_aug_loader)\n",
        "        test_accs.append(test_acc)\n",
        "        aug_accs.append(aug_acc)\n",
        "\n",
        "        msg = f\"[Run {run_id}][Epoch {epoch+1}/{num_epochs}] Test={test_acc*100:.2f}%, Aug={aug_acc*100:.2f}%\"\n",
        "        print(msg)\n",
        "\n",
        "    return np.array(test_accs), np.array(aug_accs), ww_logs_local\n",
        "\n",
        "# -----------------------------\n",
        "# Run experiments\n",
        "# -----------------------------\n",
        "baseline_runs, baseline_aug_runs, baseline_ww_logs_runs = [], [], []\n",
        "wwpgd_runs,    wwpgd_aug_runs,    wwpgd_ww_logs_runs    = [], [], []\n",
        "\n",
        "print(\"\\n=== Running BASELINE (AdamW) ===\")\n",
        "for r in range(N_RUNS):\n",
        "    accs, aug_accs, wwlogs = train_one_run(run_id=r, use_ww_pgd=False, num_epochs=NUM_EPOCHS)\n",
        "    baseline_runs.append(accs)\n",
        "    baseline_aug_runs.append(aug_accs)\n",
        "    baseline_ww_logs_runs.append(wwlogs)\n",
        "\n",
        "print(\"\\n=== Running WW-PGD (AdamW + ww_pgd) ===\")\n",
        "for r in range(N_RUNS):\n",
        "    accs, aug_accs, wwlogs = train_one_run(run_id=100 + r, use_ww_pgd=True, num_epochs=NUM_EPOCHS)\n",
        "    wwpgd_runs.append(accs)\n",
        "    wwpgd_aug_runs.append(aug_accs)\n",
        "    wwpgd_ww_logs_runs.append(wwlogs)\n",
        "\n",
        "baseline_runs     = np.stack(baseline_runs,     axis=0)\n",
        "baseline_aug_runs = np.stack(baseline_aug_runs, axis=0)\n",
        "wwpgd_runs        = np.stack(wwpgd_runs,        axis=0)\n",
        "wwpgd_aug_runs    = np.stack(wwpgd_aug_runs,    axis=0)\n",
        "\n",
        "epochs = np.arange(1, NUM_EPOCHS + 1)\n",
        "\n",
        "def mean_std(x):\n",
        "    return x.mean(axis=0), x.std(axis=0)\n",
        "\n",
        "baseline_mean, baseline_std = mean_std(baseline_runs)\n",
        "wwpgd_mean,    wwpgd_std    = mean_std(wwpgd_runs)\n",
        "\n",
        "baseline_aug_mean, baseline_aug_std = mean_std(baseline_aug_runs)\n",
        "wwpgd_aug_mean,    wwpgd_aug_std    = mean_std(wwpgd_aug_runs)\n",
        "\n",
        "\n",
        "\n",
        "plot_metric_with_errorbars(alpha_base, alpha_pgd, \"Alpha per Layer (mean ± std)\", \"alpha\")\n",
        "plot_metric_with_errorbars(xmin_base, xmin_pgd, \"xmin per Layer (mean ± std)\", \"xmin\")\n",
        "plot_metric_with_errorbars(detx_base, detx_pgd, \"detX_num per Layer (mean ± std)\", \"detX_num\")\n",
        "plot_metric_with_errorbars(trap_base, trap_pgd, \"detX_num - num_pl_spikes per Layer (mean ± std)\", \"detX_num - num_pl_spikes\")"
      ],
      "metadata": {
        "id": "unMz6iSEPNfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Plot plain test acc\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.errorbar(epochs, baseline_mean*100, yerr=baseline_std*100, marker=\"o\", linestyle=\"-\", capsize=4,\n",
        "             label=\"Baseline (plain)\")\n",
        "plt.errorbar(epochs, wwpgd_mean*100, yerr=wwpgd_std*100, marker=\"o\", linestyle=\"--\", capsize=4,\n",
        "             label=\"ww_pgd (plain)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"MNIST – Plain Test Accuracy (mean ± std)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Plot augmented test acc\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.errorbar(epochs, baseline_aug_mean*100, yerr=baseline_aug_std*100, marker=\"o\", linestyle=\"-\", capsize=4,\n",
        "             label=\"Baseline (aug)\")\n",
        "plt.errorbar(epochs, wwpgd_aug_mean*100, yerr=wwpgd_aug_std*100, marker=\"o\", linestyle=\"--\", capsize=4,\n",
        "             label=\"ww_pgd (aug)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"MNIST – Augmented Test Accuracy (mean ± std)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal (epoch {NUM_EPOCHS}) mean ± std:\")\n",
        "print(f\"Baseline: plain={baseline_mean[-1]*100:.2f}% ± {baseline_std[-1]*100:.2f}%, \"\n",
        "      f\"aug={baseline_aug_mean[-1]*100:.2f}% ± {baseline_aug_std[-1]*100:.2f}%\")\n",
        "print(f\"ww_pgd : plain={wwpgd_mean[-1]*100:.2f}% ± {wwpgd_std[-1]*100:.2f}%, \"\n",
        "      f\"aug={wwpgd_aug_mean[-1]*100:.2f}% ± {wwpgd_aug_std[-1]*100:.2f}%\")\n",
        "\n",
        "# ======================================================\n",
        "# WW metric extraction + plots\n",
        "# ======================================================\n",
        "\n",
        "def extract_metric_paths(ww_logs_runs, num_epochs, metric_name, is_trap=False):\n",
        "    \"\"\"\n",
        "    ww_logs_runs: list[ list[pd.DataFrame] ] per run\n",
        "    returns dict[layer] -> np.array shape (N_RUNS, num_epochs)\n",
        "    \"\"\"\n",
        "    N = len(ww_logs_runs)\n",
        "    layers = [\"FC1\", \"FC2\", \"FC3\"]\n",
        "    layer_paths = {L: np.full((N, num_epochs), np.nan) for L in layers}\n",
        "\n",
        "    for run_idx, wwlogs in enumerate(ww_logs_runs):\n",
        "        ww_all = pd.concat(wwlogs, ignore_index=True)\n",
        "        name_col = \"longname\" if \"longname\" in ww_all.columns else \"name\"\n",
        "        layer_map = {\"net.1\": \"FC1\", \"net.3\": \"FC2\", \"net.5\": \"FC3\"}\n",
        "        ww_all[\"layer\"] = ww_all[name_col].map(layer_map).fillna(ww_all[name_col])\n",
        "\n",
        "        spikes_col = None\n",
        "        if is_trap:\n",
        "            if \"num_pl_spikes\" in ww_all.columns:\n",
        "                spikes_col = \"num_pl_spikes\"\n",
        "            elif \"num_spikes\" in ww_all.columns:\n",
        "                spikes_col = \"num_spikes\"\n",
        "\n",
        "        for L in layers:\n",
        "            for e in range(num_epochs):\n",
        "                df = ww_all[(ww_all[\"layer\"] == L) & (ww_all[\"epoch_id\"] == e)]\n",
        "                if df.empty:\n",
        "                    continue\n",
        "                if is_trap:\n",
        "                    if spikes_col is None or \"detX_num\" not in df.columns:\n",
        "                        continue\n",
        "                    layer_paths[L][run_idx, e] = float((df[\"detX_num\"] - df[spikes_col]).mean())\n",
        "                else:\n",
        "                    if metric_name not in df.columns:\n",
        "                        continue\n",
        "                    layer_paths[L][run_idx, e] = float(df[metric_name].mean())\n",
        "\n",
        "    return layer_paths\n",
        "\n",
        "alpha_base = extract_metric_paths(baseline_ww_logs_runs, NUM_EPOCHS, \"alpha\", is_trap=False)\n",
        "alpha_pgd  = extract_metric_paths(wwpgd_ww_logs_runs,   NUM_EPOCHS, \"alpha\", is_trap=False)\n",
        "\n",
        "xmin_base  = extract_metric_paths(baseline_ww_logs_runs, NUM_EPOCHS, \"xmin\", is_trap=False)\n",
        "xmin_pgd   = extract_metric_paths(wwpgd_ww_logs_runs,   NUM_EPOCHS, \"xmin\", is_trap=False)\n",
        "\n",
        "detx_base  = extract_metric_paths(baseline_ww_logs_runs, NUM_EPOCHS, \"detX_num\", is_trap=False)\n",
        "detx_pgd   = extract_metric_paths(wwpgd_ww_logs_runs,   NUM_EPOCHS, \"detX_num\", is_trap=False)\n",
        "\n",
        "trap_base  = extract_metric_paths(baseline_ww_logs_runs, NUM_EPOCHS, metric_name=None, is_trap=True)\n",
        "trap_pgd   = extract_metric_paths(wwpgd_ww_logs_runs,   NUM_EPOCHS, metric_name=None, is_trap=True)\n",
        "\n",
        "baseline_colors = {\"FC1\":\"#ffb3b3\",\"FC2\":\"#ff6666\",\"FC3\":\"#cc0000\"}\n",
        "pgd_colors      = {\"FC1\":\"#b3d1ff\",\"FC2\":\"#6699ff\",\"FC3\":\"#004c99\"}\n",
        "layers = [\"FC1\",\"FC2\",\"FC3\"]\n",
        "\n",
        "def plot_metric_with_errorbars(metric_base, metric_pgd, title, ylabel):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    for L in layers:\n",
        "        b = metric_base[L]\n",
        "        p = metric_pgd[L]\n",
        "        b_mean, b_std = np.nanmean(b, axis=0), np.nanstd(b, axis=0)\n",
        "        p_mean, p_std = np.nanmean(p, axis=0), np.nanstd(p, axis=0)\n",
        "\n",
        "        plt.errorbar(epochs, b_mean, yerr=b_std, marker=\"o\", linestyle=\"-\", capsize=3,\n",
        "                     color=baseline_colors[L], label=f\"{L} Baseline\")\n",
        "        plt.errorbar(epochs, p_mean, yerr=p_std, marker=\"o\", linestyle=\"--\", capsize=3,\n",
        "                     color=pgd_colors[L], label=f\"{L} ww_pgd\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sgw3ZAYsPLry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHkHF6KNTPgI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNu/nkHHuHVi71KsFs/SFNO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}